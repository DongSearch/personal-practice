{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epaJdvVx9BTE",
        "outputId": "e7e2a7d5-c658-4e08-f12c-4074680fec02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Collecting cairosvg\n",
            "  Downloading cairosvg-2.8.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting cairocffi (from cairosvg)\n",
            "  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting cssselect2 (from cairosvg)\n",
            "  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from cairosvg) (0.7.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from cairosvg) (1.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from cairocffi->cairosvg) (1.17.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n",
            "Downloading cairosvg-2.8.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: cssselect2, cairocffi, cairosvg\n",
            "Successfully installed cairocffi-1.7.1 cairosvg-2.8.2 cssselect2-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers pillow cairosvg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTImageProcessor,ViTForImageClassification,AutoFeatureExtractor, AutoModelForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "import torch.nn.functional as F\n",
        "import cairosvg"
      ],
      "metadata": {
        "id": "L99sP3j49J_z"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://images.unsplash.com/photo-1589571894960-20bbe2828d0a\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n"
      ],
      "metadata": {
        "id": "2OcFmle09pGc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract feature from image using the pre-fiend extractor\n",
        "extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "# pre-processing image\n",
        "input = extractor(images=image, return_tensors=\"pt\")\n",
        "# it is a preprocessed model\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "#do inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)"
      ],
      "metadata": {
        "id": "ErtoWEgs-EfL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output\n",
        "# after finishing prediction, they return to each score\n",
        "scores = outputs.logits\n",
        "prediction = scores.argmax(-1).item()\n",
        "print(\"prediction :\", model.config.id2label[prediction])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP4km8R9AJat",
        "outputId": "1543f3f7-1ee8-444a-dd6a-989e691ac8d5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction : park bench\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Even though the lady is on the center, they predict the bench that barely show"
      ],
      "metadata": {
        "id": "H5E_kFS9BRH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict 5 classes\n",
        "top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aObZqRMBENP",
        "outputId": "abe85f05-ec50-4837-cc56-758152fcd79a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. park bench                 확률: 33.36%\n",
            "2. ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin  확률: 23.00%\n",
            "3. cowboy boot                확률: 1.89%\n",
            "4. barrow, garden cart, lawn cart, wheelbarrow  확률: 1.72%\n",
            "5. folding chair              확률: 1.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same thing with CNNs instead of ViT to compare their performances\n",
        "\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
        "\n",
        "input = extractor(images = image, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)\n",
        "    scores = outputs.logits\n",
        "    top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-Cg4W1iB6Jk",
        "outputId": "aebfa2f9-10e4-4ecd-ebbb-6896ff2c9ed6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. hay                        확률: 20.07%\n",
            "2. rapeseed                   확률: 13.96%\n",
            "3. bikini, two-piece          확률: 11.93%\n",
            "4. cloak                      확률: 4.23%\n",
            "5. fur coat                   확률: 3.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_checkerboard(tile_size=32, num_tiles=8):\n",
        "    row_even = np.array([0, 1] * (num_tiles // 2))\n",
        "    row_odd = np.array([1, 0] * (num_tiles // 2))\n",
        "    board = np.row_stack([row_even if i % 2 == 0 else row_odd for i in range(num_tiles)])\n",
        "    board = np.kron(board, np.ones((tile_size, tile_size))) * 255\n",
        "    return Image.fromarray(board.astype('uint8'))\n",
        "\n",
        "image = create_checkerboard()\n",
        "image = image.convert(\"RGB\")\n",
        "display(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "6Yk--8E9DtBK",
        "outputId": "aac7db40-60af-4728-e2c8-3a3b716793c7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-69-3685451971.py:6: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
            "  board = np.row_stack([row_even if i % 2 == 0 else row_odd for i in range(num_tiles)])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=256x256>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "# pre-processing image\n",
        "input = processor(images=[image], return_tensors=\"pt\")\n",
        "# it is a preprocessed model\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "#do inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)\n",
        "    # output\n",
        "# after finishing prediction, they return to each score\n",
        "scores = outputs.logits\n",
        "prediction = scores.argmax(-1).item()\n",
        "print(\"prediction :\", model.config.id2label[prediction])\n",
        "# predict 5 classes\n",
        "top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "\n",
        "print(\"ViT\")\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
        "\n",
        "input = extractor(images = [image], return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)\n",
        "    scores = outputs.logits\n",
        "    top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "print(\"CNN\")\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT2apOsqJdiU",
        "outputId": "f4263bbb-80c3-401d-bc18-bb7028bf006c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction : crossword puzzle, crossword\n",
            "ViT\n",
            "1. crossword puzzle, crossword  확률: 46.64%\n",
            "2. maze, labyrinth            확률: 1.44%\n",
            "3. jigsaw puzzle              확률: 1.37%\n",
            "4. lens cap, lens cover       확률: 0.77%\n",
            "5. doormat, welcome mat       확률: 0.76%\n",
            "CNN\n",
            "1. web site, website, internet site, site  확률: 4.35%\n",
            "2. abaya                      확률: 1.47%\n",
            "3. nematode, nematode worm, roundworm  확률: 1.34%\n",
            "4. theater curtain, theatre curtain  확률: 1.33%\n",
            "5. panpipe, pandean pipe, syrinx  확률: 1.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://images.pexels.com/photos/127028/pexels-photo-127028.jpeg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "# pre-processing image\n",
        "input = extractor(images=image, return_tensors=\"pt\")\n",
        "# it is a preprocessed model\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "#do inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)\n",
        "    # output\n",
        "# after finishing prediction, they return to each score\n",
        "scores = outputs.logits\n",
        "prediction = scores.argmax(-1).item()\n",
        "print(\"prediction :\", model.config.id2label[prediction])\n",
        "# predict 5 classes\n",
        "top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "\n",
        "print(\"ViT\")\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
        "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
        "\n",
        "input = extractor(images = image, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**input)\n",
        "    scores = outputs.logits\n",
        "    top5 = torch.topk(F.softmax(scores,dim=-1), k =5)\n",
        "print(\"CNN\")\n",
        "for idx, (prob, label_id) in enumerate(zip(top5.values[0], top5.indices[0])):\n",
        "    label_name = model.config.id2label[label_id.item()]\n",
        "    print(f\"{idx+1}. {label_name:<25}  확률: {prob.item() * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC5nWh2CJ8cd",
        "outputId": "bd0342fe-fb76-403b-9a4f-9140100c2f80"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction : lynx, catamount\n",
            "ViT\n",
            "1. lynx, catamount            확률: 36.29%\n",
            "2. Persian cat                확률: 29.73%\n",
            "3. Egyptian cat               확률: 9.74%\n",
            "4. tabby, tabby cat           확률: 3.81%\n",
            "5. Siamese cat, Siamese       확률: 3.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/convnext/feature_extraction_convnext.py:30: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN\n",
            "1. lynx, catamount            확률: 87.46%\n",
            "2. Egyptian cat               확률: 4.51%\n",
            "3. Persian cat                확률: 1.71%\n",
            "4. tiger cat                  확률: 0.26%\n",
            "5. remote control, remote     확률: 0.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this case, CNN has better performance than ViT, because CNN is stronger to capture local features, especially face, fur, eyes of cat, while Vit split the image to token and they are fed up to transformer(encoder), which capture more global part. Therefore, we can't generalize that ViT has always better performance"
      ],
      "metadata": {
        "id": "V4eoXKL1Mt-e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYBNMi3BNUYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}